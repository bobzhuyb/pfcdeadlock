%\vspace{-0.1in}
\section{\sys Applications}\label{sec:application}
%\vspace{-0.1in}
To showcase \sys's utility, we use it for explicit path support in four applications. The key is that, built on \sys, applications can freely choose which path to use without worrying about how to set up the path and the time cost or overhead of setting up the path. In this regard, \sys emerges as an interface for applications to use explicit paths conveniently, but does not make any choice on behalf of them.

%We incorporate \sys into 5 representative applications for routing and demonstrate its utility. For bandwidth-guarantee, we show that \sys's explicit path control makes end-to-end bandwidth provisioning easier to implement. For partition-aggregation application, we show that \sys can express 1-to-$n$ or $n$-to-1 patterns and can be leveraged to give search query high priority for service differentiation. For map-reduce, we show that \sys can express $m$-to-$n$ pattern and the path IDs can used to identify disjoint parallel paths that speedup the many-to-many data shuffle and avoid collision of ECMP.

%\begin{figure}[t]
%\centering
%\subfloat[nooneline, footnotesize][Remaining bandwidth on $P_1$, $P_2$, $P_3$ is 300, 100, 100 Mbps.]
%{\includegraphics[width=0.28\textwidth] {figs/iops-topo.eps}}
%%\vspace{-0.1in}
%\subfloat[nooneline, footnotesize][Average IOPS.] {
%{\scalebox{0.8} {
%\begin{tabular}[b]{|l | c|}
%    \hline
%    & \tabincell{c}{\textbf{Average IOPS}} \\
%    \hline
%    \hline
%    \sys & 15274 \\
%    \hline
%    ECMP & 4547 \\
%    \hline
%\end{tabular}}}}
%
%\subfloat[nooneline, footnotesize][Throughput and completion time of RoX and ECMP.] {
%\includegraphics[width=0.45\textwidth]{figs/iops-throughput.eps}}
%%\vspace{-0.17in}
%\caption{\sys utility case $\#1$: we leverage \sys to make provisioned I/O bandwidth easier to implement.}
%\end{figure}

%\vspace{-0.1in}
\subsection{\sys for provisioned IOPS}\label{subsec:iops}
%\vspace{-0.1in}
%In cloud services, there is an increasing need for bandwidth provisioning. For example, Amazon RDS enforces provisioned I/O bandwidth for instances to ensure that disk resources are always available whenever you need them~\cite{privisionIO}. In this experiment, we show \sys's explicit path control makes end-to-end bandwidth provisioning easy to implement. For the experiment setting, we pick a pod and use background UDP flows to stature the ToR-Agg links and leave the remaining bandwidth on three paths between X-Y as 100Mpbs, 300Mbps, 100Mbps respectively. Suppose now there comes a demand to provision the bandwidth between X-Y at 300Mbps. We then leverage ECMP and \sys to implement it.

\begin{figure}[t]
\centering
%\vspace{-0.1in}
\includegraphics[width=0.5\textwidth,center]{figs/f8.eps}\hspace{-0.2in}
%\vspace{-0.1in}
\caption[Optional caption for list of figures]{\sys utility case $\#1$: we leverage \sys to make necessary bandwidth easier to implement for provisioned IOPS.}
\label{fig:iops}
\vspace{-0.2in}
\end{figure}

In cloud services, there is an increasing need for provisioned IOPS. For example, Amazon EBS enforces provisioned IOPS for instances to ensure that disk resources can be accessed with high and consistent I/O performance whenever you need them~\cite{privisionIO-EBS}. To enforce such provisioned IOPS, it should first provide necessary bandwidth for the instances~\cite{io-characteristics}. In this experiment, we show \sys can be easily leveraged to use the explicit path with necessary bandwidth.

As shown in Fig.~\ref{fig:iops}(a), we use background UDP flows to stature the ToR-Agg links and leave the remaining bandwidth on 3 paths ($P_1, P_2$ and $P_3$) between X-Y as 300Mpbs, 100Mbps, and 100Mbps respectively. Suppose there is a request for provisioned IOPS that requires 500Mbps necessary bandwidth (The provisioned IOPS is about 15000 and the chunk size is 4KB.). We now leverage \sys and ECMP to write 15GB data ($\approx$4 million chunks) through 30 flows from X to Y, and measure the achieved IOPS respectively. The storage we used for the experiment is Kingston V+200 120G SSD, and the I/O operations on the storage are sequential read and sequential write.

\begin{figure}[t]
\centering
\vspace{-0.15in}
\subfloat[short for lof][Path $P_1$: T1~$\rightarrow$A1~$\rightarrow$T3; $P_2$: T1~$\rightarrow$A2~$\rightarrow$T3; $P_3$: T1~$\rightarrow$A3~$\rightarrow$T3] {
    \includegraphics[width=0.40\textwidth] {figs/updt-topo.eps}
    \label{fig:network-update_a}
}
\vspace{-0.15in}

\subfloat[short for lof][Time $t_1$: move $f_3$ from $P_2$ to $P_3$;~ $t_2$: move $f_1$ from $P_1$ to $P_2$;~$t_3$: move $f_1$ from $P_2$ to $P_1$;~ $t_4$: move $f_3$ from $P_3$ to $P_2$.]{
    \includegraphics[width=0.40\textwidth] {figs/updt-mat-new.eps}
    \label{fig:network-update_b}
}
\vspace{-0.1in}
\caption[Optional caption for list of figures]{\sys utility case \#2: we leverage XPath to assist zUpdate~\cite{zupdate} to accomplish DCN update with zero loss.}
\label{fig:network-update}
\vspace{-0.2in}
\end{figure}

From Fig.~\ref{fig:iops}(c), it can be seen that using ECMP we cannot provide the necessary bandwidth between X-Y for the provisioned IOPS although the physical capacity is there. Thus, the actual achieved IOPS is only 4547, and the write under ECMP takes much longer time than that under \sys as shown in Fig.~\ref{fig:iops}(c). This is because ECMP performs random hashing and cannot specify the explicit path to use, hence it cannot accurately make use of the remaining bandwidth on each of the multiple paths for end-to-end bandwidth provisioning. In contrast, \sys can be easily leveraged to provide the required bandwidth due to its explicit path control. With \sys, we explicitly control how to use the three paths and accurately provide 500Mbps necessary bandwidth, achieving 15274 IOPS.


%\vspace{-0.05in}
\subsection{\sys for network updating}\label{subsec:update}
\vspace{-0.05in}
In production data centers, DCN update occurs frequently~\cite{zupdate}. It can be triggered by the operators, applications and various networking failures. zUpdate~\cite{zupdate} is an application that aims to perform congestion-free network-wide traffic migration during DCN updates with zero loss and zero human effort. In order to achieve its goal, zUpdate requires explicit routing path control over the underlying DCNs. In this experiment, we show how \sys assists zUpdate to accomplish DCN update and use a switch firmware upgrade example to show how traffic migration is conducted with \sys.

In Fig.~\ref{fig:network-update}(a), initially we assume 4 flows ($f_1, f_2, f_3$ and $f_4$) on three paths ($P_1, P_2$ and $P_3$). Then we move $f_1$ away from switch $A_1$ to do a firmware upgrade for switch $A_1$. However, neither $P_2$ nor $P_3$ has enough spare bandwidth to accommodate $f_1$ at this point of time. Therefore we need to move $f_3$ from $P_2$ to $P_3$ in advance. Finally, after the completion of firmware upgrade, we move all the flows back to original paths. We leverage \sys to implement the whole movement.

\begin{figure}[t]
\centering
%\vspace{-0.2in}
\includegraphics[width=0.45\textwidth]{figs/be.eps}
%\vspace{-0.15in}
\caption{\sys utility case $\#3$: we leverage \sys to accurately enforce VDC with bandwidth guarantees.}
\label{fig:bandwidth_enforcement}
\vspace{-0.2in}
\end{figure}

In Fig.~\ref{fig:network-update}(b), we depict the link utilization dynamics. At time $t_1$, when $f_3$ is moved from $P_2$ to $P_3$, the link utilization of $P_2$ drops from 0.6 to 0.4 and the link utilization of $P_3$ increases from 0.7 to 0.9. At time $t_2$, when $f_1$ is moved from $P_1$ to $P_2$, the link utilization of $P_1$ drops from 0.5 to 0 and the link utilization of $P_2$ increases from 0.4 to 0.9. The figure also shows the changes of the link utilization at time $t_3$ and $t_4$ when moving $f_3$ back to $P_2$ and $f_1$ back to $P_1$. It is easy to see that with the help of \sys, $P_1, P_2$ and $P_3$ see no congestion and DCN update proceeds smoothly without loss.

%Compared to OpenFlow, since \sys is capable to pre-install large number of paths into IP tables, zUpdate doesn't need to change the flow and group tables on switches in every step of the traffic migration. For most of the cases, \sys only needs to notify the senders to transfer a part of flows to some other paths.

%\vspace{-0.1in}
\subsection{Virtual network enforcement with \sys}\label{subsec:vne}
%\vspace{-0.05in}
%
%\begin{figure*}[ht]
%\centering
%\subfloat[short for lof][The Fattree(6) testbed with 54 servers we implemented. Each ToR switch connects 3 servers (not drawn).] {
%    \includegraphics[width=0.32\textwidth] {figs/testbed-topo}
%    \label{fig:fattree}
%}
%\vspace{-0.17in}\hfill
%\subfloat[short for lof][\sys utility case $\#4$: we leverage \sys to express partition-aggregation traffic pattern ($1$-to-$n$, $n$-to-$1$) and enforce service differentiation.] {
%    \includegraphics[width=0.33\textwidth] {figs/pa.eps}
%    \label{fig:partition_aggregation_query}
%}\hfill
%\subfloat[short for lof][\sys utility case $\#5$: we leverage \sys to select non-conflict paths to speed up many-to-many data shuffle.]{
%    \includegraphics[width=0.3\textwidth] {figs/shuffle}
%    \label{fig:shuffle}
%}
%\caption[Optional caption for list of figures]{}
%\label{fig:network-update}
%\vspace{-0.2in}
%\end{figure*}


In cloud computing, virtual data center (VDC) abstraction with bandwidth guarantees is an appealing model due to its performance predictability in shared environments~\cite{secondnet,oktopus,proteus2012}. In this experiment, we show \sys can be applied to enforce virtual networks with bandwidth guarantees. We assume a simple SecondNet-based VDC model with 4 virtual links, and the bandwidth requirements on them are 50Mbps, 200Mbps, 250Mbps and 400Mbps respectively as shown in Fig.~\ref{fig:bandwidth_enforcement}(a). We then leverage \sys's explicit path control to embed this VDC into the physical topology.

In Fig.~\ref{fig:bandwidth_enforcement}(b), we show that \sys can easily be employed to use the explicit paths in the physical topology with enough bandwidth to embed the virtual links. In Fig.~\ref{fig:bandwidth_enforcement}(c), we measure the actual bandwidth for each virtual link and show that the desired bandwidth is accurately enforced. However, we found that ECMP cannot be used to accurately enable this because ECMP cannot control paths explicitly.

%\vspace{-0.2in}
%\subsection{\sys support for partition-aggregation query}\label{subsec:partition}
%\vspace{-0.1in}
%In web applications, the partition-aggregation paradigm is a foundation for many online services such as search query. They usually generate one-to-many and many-to-one communication patterns and has very demanding latency requirements. Using \sys, we can explicitly express such 1-to-$n$ and $n$-to-1 patterns using ($n$+1) path IDs, one ID for $n$-to-1 and $n$ IDs for 1-to-$n$. These IDs form a group that can be leveraged for optimizations such as service differentiation.
%
%In this experiment, we selected 9 servers in a pod to emulate a 1-to-8 (8-to-1) query-and-response, and we used 9 IDs to express such group communication patterns. We saturate the network with background traffic, and then leverage such 9 IDs to set priority to such query-and-response traffic.
%
%\begin{figure}[t]
%\vspace{-0.1in}
%\centering
%\includegraphics[width=0.45\textwidth]{figs/pa.eps}
%\vspace{-0.1in}
%\caption{\sys utility case $\#4$: we leverage \sys to express partition-aggregation traffic pattern ($1$-to-$n$, $n$-to-$1$) and enforce service differentiation.}
%\label{fig:partition_aggregation_query}
%\vspace{-0.25in}
%\end{figure}
%
%
%
%In Fig.~\ref{fig:partition_aggregation_query}, we found that when the group of path IDs are referenced for priority, the query flows see persistently low RTTs of less than 200$\mu$s irrespective of the background traffic. However, if we do not set up a priority for these IDs, the RTT increases to the millisecond level as the background load increases. This experiment showcases \sys's utility in service differentiation for partition-aggregation queries.

%\vspace{-0.1in}
\subsection{Map-reduce data shuffle with \sys}\label{mapreduce}
%\vspace{-0.1in}
In Map-reduce applications, many-to-many data shuffle between the map and reduce stages can be time-consuming. For example, Hadoop traces from Facebook show that, on average, transferring data between successive stages accounts for $33\%$ of the running times of jobs~\cite{orchestra}. Using \sys, we can explicitly express non-conflict parallel paths to speed up such many-to-many data shuffle. Usually, for a $m$-to-$n$ data shuffle, we can use ($m$+$n$) path IDs to express the communication patterns. The shuffle patterns can be predicted using existing techniques~\cite{hadoopwatch}.

\begin{figure}[t]
%\vspace{-0.1in}
\centering
\includegraphics[width=0.5\textwidth]{figs/shuffle}
\vspace{-0.2in}
\caption{\sys utility case $\#4$: we leverage \sys to select non-conflict paths to speed up many-to-many data shuffle.}
\label{fig:shuffle}
\vspace{-0.2in}
\end{figure}

In this experiment, we selected 18 servers in two pods of the Fattree to emulate a 9-to-9 data shuffle by letting 9 servers in one pod send data to 9 servers in the other pod. We varied the data volume from 40G to over 400G. We compared \sys with ECMP.

In Fig.~\ref{fig:shuffle}, it can be seen that by using \sys for data shuffle, we can perform considerably better than randomized ECMP hash-based routing. More specifically, it reduces the shuffle time by over 3$\times$ for most of the experiments. The reason is that \sys's explicit path IDs can be easily leveraged to arrange non-interfering paths for shuffling, thus the network bisection bandwidth is fully utilized for speedup.


%We further note that \sys can simply use 3 path IDs to express such many-to-many communication patterns, which makes the non-conflict parallel path arrangement fairly easy to implement.


